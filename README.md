# maths-prompt

Can iterative prompt engineering teach a raw base LLM to do maths?

Claude (via the Anthropic API) acts as the optimiser. Its only lever is rewriting the system prompt, and its only feedback is an accuracy score on freshly randomised problems each time.

## Design

- **No LLM scoring** — correctness is checked with pure Python arithmetic (regex + float compare)
- **Randomised training problems** (400 per eval) prevent prompt overfitting to specific questions
- **Separate test set** with structurally different problem types — Claude never sees these; they measure generalisation
- **Information asymmetry** — Claude only sees accuracy %. Everything else (prompts, model outputs, per-problem results) is logged locally
- **Free inference** — the model being prompted runs locally via mlx-lm (native Apple Silicon batch inference)

## Architecture

```
main.py (daemon: start/stop/status/logs)
  └─ runner.py ──► Anthropic API (tool use)
                        └─ evaluator.py (local tool execution)
                             ├─ generator.py  (random problems)
                             ├─ model.py      (mlx-lm, batch)
                             └─ scorer.py     (pure Python)
```

Claude has exactly one tool: `evaluate_prompt(prompt) → "Accuracy: 35%"`. It calls this in a loop, iterating on the prompt.

## Model

`Qwen2.5-0.5B` (4-bit quantised) — a tiny base model with no RLHF and no instruction tuning. It cannot do maths by default. Inference runs natively on Apple Silicon via MLX.

## Problem types

**Training** (randomised each eval, Claude evaluates on these):
- Simple: `347 + 589`, `23 * 17`
- Medium: `12 + 34 * 5`, `(48 - 12) / 6`
- Hard: `(15 + 7) * (3 - 1)`, `((8 + 2) * 5) / 10`

**Test** (generated with fixed seed, Claude never sees these — measures generalisation):
- Exponents: `2 ** 8`
- Modulo: `47 % 7`
- Long chains: `2 + 3 + 5 + 7 + 11 + 13`
- Deeply nested: `((2 + 3) * (4 - 1)) / (6 - 1)`
- Negatives: `(-5) * 3 + 12`
- Decimals: `2.5 * 4 + 1.5`
- Large numbers: `1234 + 5678`

## Setup

Requires Python 3.11+, [uv](https://docs.astral.sh/uv/), and a valid Anthropic API key. Apple Silicon Mac required (mlx-lm uses Metal).

```bash
# Install dependencies
uv sync

# Set your Anthropic API key
export MATHS_PROMPT_API_KEY=sk-ant-...

# One-time: convert the model to 4-bit MLX format (~300 MB, downloads from HuggingFace)
uv run python -m mlx_lm.convert --hf-path Qwen/Qwen2.5-0.5B --mlx-path models/Qwen2.5-0.5B-4bit -q

# Start the optimizer daemon
uv run maths-prompt start

# Watch the output
uv run maths-prompt logs

# Check status and best accuracy
uv run maths-prompt status

# Stop the daemon
uv run maths-prompt stop
```

## Logs

Every `evaluate_prompt` call is logged to `logs/evaluations.jsonl` with the full prompt, all 400 problems, model responses, and per-problem results. Test set evaluations go to `logs/test_results.jsonl`.

```bash
# Training accuracy over time
python -c "import json; [print(f\"{json.loads(l)['iteration']}: {json.loads(l)['accuracy']:.1%}\") for l in open('logs/evaluations.jsonl')]"

# Open the Streamlit dashboard
uv run streamlit run src/maths_prompt/dashboard.py
```

## Project structure

```
maths-prompt/
├── src/maths_prompt/
│   ├── config.py          # Settings: model, paths, counts
│   ├── generator.py       # Random arithmetic expression generator
│   ├── model.py           # mlx-lm interface (single + batch inference)
│   ├── scorer.py          # Regex extraction + float comparison
│   ├── evaluator.py       # Prompt evaluation (generate, query, score, log)
│   ├── runner.py          # Anthropic API tool-use optimizer
│   ├── test_eval.py       # Held-out test set evaluator
│   ├── dashboard.py       # Streamlit progress dashboard
│   └── main.py            # CLI entrypoint (start/stop/status/logs)
├── models/                  # MLX model weights (gitignored, generated by convert)
├── logs/                    # JSONL logs + daemon output (gitignored except .gitkeep)
└── notebooks/explore.ipynb  # Exploration notebook for base model experiments
```
