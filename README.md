# maths-prompt

Can iterative prompt engineering teach a raw base LLM to do maths?

A sandboxed Claude Code instance acts as the optimiser. Its only lever is rewriting the system prompt, and its only feedback is an accuracy score on freshly randomised problems each time.

## Design

- **No LLM scoring** — correctness is checked with pure Python arithmetic (regex + float compare)
- **Randomised training problems** (400 per eval) prevent prompt overfitting to specific questions
- **Separate test set** with structurally different problem types — Claude never sees these; they measure generalisation
- **Information asymmetry** — Claude only sees accuracy %. Everything else (prompts, model outputs, per-problem results) is logged locally
- **Free inference** — the model being prompted runs locally via mlx-lm (native Apple Silicon batch inference)

## Architecture

```
main.py (runner)
  └─ subprocess ──► sandboxed Claude Code
                        └─ MCP stdio ──► mcp_server.py
                                             ├─ generator.py  (random problems)
                                             ├─ model.py      (mlx-lm, batch)
                                             └─ scorer.py     (pure Python)
```

Claude has exactly one tool: `evaluate_prompt(prompt) → "Accuracy: 35%"`. It calls this in a loop, iterating on the prompt.

## Model

`Qwen2.5-0.5B` (4-bit quantised) — a tiny base model with no RLHF and no instruction tuning. It cannot do maths by default. Inference runs natively on Apple Silicon via MLX.

## Problem types

**Training** (randomised each eval, Claude evaluates on these):
- Simple: `347 + 589`, `23 * 17`
- Medium: `12 + 34 * 5`, `(48 - 12) / 6`
- Hard: `(15 + 7) * (3 - 1)`, `((8 + 2) * 5) / 10`

**Test** (fixed, Claude never sees these — measures generalisation):
- Exponents: `2 ** 8`
- Modulo: `47 % 7`
- Long chains: `2 + 3 + 5 + 7 + 11 + 13`
- Deeply nested: `((2 + 3) * (4 - 1)) / (6 - 1)`
- Negatives: `(-5) * 3 + 12`
- Decimals: `2.5 * 4 + 1.5`
- Large numbers: `1234 + 5678`

## Setup

Requires Python 3.11+, [uv](https://docs.astral.sh/uv/), and a valid `ANTHROPIC_API_KEY`. Apple Silicon Mac required (mlx-lm uses Metal).

```bash
# Install dependencies
uv sync

# One-time: convert the model to 4-bit MLX format (~300 MB, downloads from HuggingFace)
uv run python -m mlx_lm convert --hf-path Qwen/Qwen2.5-0.5B --mlx-path models/Qwen2.5-0.5B-4bit -q

# Run the optimizer
uv run maths-prompt
```

## Logs

Every `evaluate_prompt` call is logged to `logs/evaluations.jsonl` with the full prompt, all 400 problems, model responses, and per-problem results. Test set evaluations go to `logs/test_results.jsonl`.

```bash
# Training accuracy over time
python -c "import json; [print(f\"{json.loads(l)['iteration']}: {json.loads(l)['accuracy']:.1%}\") for l in open('logs/evaluations.jsonl')]"

# Open the Streamlit dashboard
uv run streamlit run src/maths_prompt/dashboard.py
```

## Project structure

```
maths-prompt/
├── src/maths_prompt/
│   ├── config.py          # Settings: model, paths, counts
│   ├── generator.py       # Random arithmetic expression generator
│   ├── model.py           # mlx-lm interface (single + batch inference)
│   ├── scorer.py          # Regex extraction + float comparison
│   ├── mcp_server.py      # FastMCP server (the only tool Claude can use)
│   ├── runner.py          # Sandboxed Claude Code launcher
│   ├── test_eval.py       # Held-out test set evaluator
│   └── main.py            # CLI entrypoint with retry loop
├── models/                  # MLX model weights (gitignored, generated by convert)
├── data/test_problems.json  # Fixed test set (39 problems, 7 categories)
├── logs/                    # JSONL logs (gitignored except .gitkeep)
├── mcp.json                 # MCP server config
└── sandbox_settings.json    # Claude Code sandbox settings
```
