{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Math Exploration\n",
    "\n",
    "Experiment with `qwen2.5:0.5b` on training problems. Inspect problem types, run the model, and see where it succeeds or fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from maths_prompt.generator import generate_problems, generate_test_problems\n",
    "from maths_prompt.model import query_model\n",
    "from maths_prompt.scorer import extract_number, check_answer\n",
    "from maths_prompt.config import OLLAMA_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inspect training problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = generate_problems(20)\n",
    "\n",
    "print(f\"{'Question':<40} {'Answer':>10}\")\n",
    "print(\"-\" * 52)\n",
    "for p in train:\n",
    "    print(f\"{p.question:<40} {p.answer:>10.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect test problems (by category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test = generate_test_problems(70)  # 10 per category\n",
    "by_cat = defaultdict(list)\n",
    "for p in test:\n",
    "    by_cat[p.category].append(p)\n",
    "\n",
    "for cat, problems in by_cat.items():\n",
    "    print(f\"\\n=== {cat} ===\")\n",
    "    for p in problems[:3]:\n",
    "        print(f\"  {p.question}  →  {p.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the model on a few problems (no system prompt)\n",
    "\n",
    "This is the raw baseline — no guidance at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"  # empty — raw base model\n",
    "\n",
    "sample = train[:10]\n",
    "results = []\n",
    "\n",
    "for p in sample:\n",
    "    response = query_model(SYSTEM_PROMPT, p.question)\n",
    "    extracted = extract_number(response)\n",
    "    correct = check_answer(extracted, p.answer)\n",
    "    results.append({\n",
    "        \"question\": p.question,\n",
    "        \"expected\": p.answer,\n",
    "        \"response\": response.strip(),\n",
    "        \"extracted\": extracted,\n",
    "        \"correct\": correct,\n",
    "    })\n",
    "\n",
    "accuracy = sum(r[\"correct\"] for r in results) / len(results)\n",
    "print(f\"Accuracy (no prompt): {accuracy:.0%}\\n\")\n",
    "\n",
    "for r in results:\n",
    "    mark = \"✓\" if r[\"correct\"] else \"✗\"\n",
    "    print(f\"{mark}  Q: {r['question']}\")\n",
    "    print(f\"   Expected: {r['expected']}  |  Extracted: {r['extracted']}\")\n",
    "    print(f\"   Model: {r['response'][:120]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Try a prompt and compare\n",
    "\n",
    "Edit `MY_PROMPT` and re-run this cell to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_PROMPT = \"\"\"Solve the math expression and output only the number.\"\"\"\n",
    "\n",
    "sample = generate_problems(20)\n",
    "results = []\n",
    "\n",
    "for p in sample:\n",
    "    response = query_model(MY_PROMPT, p.question)\n",
    "    extracted = extract_number(response)\n",
    "    correct = check_answer(extracted, p.answer)\n",
    "    results.append({\n",
    "        \"question\": p.question,\n",
    "        \"expected\": p.answer,\n",
    "        \"response\": response.strip(),\n",
    "        \"extracted\": extracted,\n",
    "        \"correct\": correct,\n",
    "    })\n",
    "\n",
    "accuracy = sum(r[\"correct\"] for r in results) / len(results)\n",
    "print(f\"Accuracy with prompt: {accuracy:.0%}\\n\")\n",
    "\n",
    "for r in results:\n",
    "    mark = \"✓\" if r[\"correct\"] else \"✗\"\n",
    "    print(f\"{mark}  {r['question']}  →  expected {r['expected']}, got {r['extracted']}\")\n",
    "    if not r[\"correct\"]:\n",
    "        print(f\"   Model said: {r['response'][:120]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate across all test categories\n",
    "\n",
    "Run your prompt against the fixed test set and see per-category accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuses MY_PROMPT from cell 4 — run that first\n",
    "test_problems = generate_test_problems(140)  # 20 per category\n",
    "by_cat = defaultdict(list)\n",
    "for p in test_problems:\n",
    "    by_cat[p.category].append(p)\n",
    "\n",
    "print(f\"{'Category':<18} {'Correct':>8} {'Total':>6} {'Acc':>6}\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "overall_correct = 0\n",
    "overall_total = 0\n",
    "\n",
    "for cat, problems in sorted(by_cat.items()):\n",
    "    correct = 0\n",
    "    for p in problems:\n",
    "        response = query_model(MY_PROMPT, p.question)\n",
    "        extracted = extract_number(response)\n",
    "        if check_answer(extracted, p.answer):\n",
    "            correct += 1\n",
    "    acc = correct / len(problems)\n",
    "    overall_correct += correct\n",
    "    overall_total += len(problems)\n",
    "    print(f\"{cat:<18} {correct:>8} {len(problems):>6} {acc:>6.0%}\")\n",
    "\n",
    "print(\"-\" * 42)\n",
    "print(f\"{'TOTAL':<18} {overall_correct:>8} {overall_total:>6} {overall_correct/overall_total:>6.0%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
