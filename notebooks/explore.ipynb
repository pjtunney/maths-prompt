{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Base Model Math Exploration\n\nExperiment with `Qwen2.5-0.5B` (4-bit MLX) on training problems. Inspect problem types, run the model, and see where it succeeds or fails."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '../src')\n\nfrom maths_prompt.generator import generate_problems, generate_test_problems\nfrom maths_prompt.model import query_model, query_model_batch\nfrom maths_prompt.scorer import extract_number, check_answer\nfrom maths_prompt.config import MLX_MODEL_PATH\n\nprint(f\"Model: {MLX_MODEL_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inspect training problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = generate_problems(20)\n",
    "\n",
    "print(f\"{'Question':<40} {'Answer':>10}\")\n",
    "print(\"-\" * 52)\n",
    "for p in train:\n",
    "    print(f\"{p.question:<40} {p.answer:>10.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect test problems (by category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test = generate_test_problems(70)  # 10 per category\n",
    "by_cat = defaultdict(list)\n",
    "for p in test:\n",
    "    by_cat[p.category].append(p)\n",
    "\n",
    "for cat, problems in by_cat.items():\n",
    "    print(f\"\\n=== {cat} ===\")\n",
    "    for p in problems[:3]:\n",
    "        print(f\"  {p.question}  →  {p.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the model on a few problems (no system prompt)\n",
    "\n",
    "This is the raw baseline — no guidance at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SYSTEM_PROMPT = \"\"  # empty — raw base model\n\nsample = train[:10]\nresponses = query_model_batch(SYSTEM_PROMPT, [p.question for p in sample])\nresults = []\n\nfor p, response in zip(sample, responses):\n    extracted = extract_number(response)\n    correct = check_answer(extracted, p.answer)\n    results.append({\n        \"question\": p.question,\n        \"expected\": p.answer,\n        \"response\": response.strip(),\n        \"extracted\": extracted,\n        \"correct\": correct,\n    })\n\naccuracy = sum(r[\"correct\"] for r in results) / len(results)\nprint(f\"Accuracy (no prompt): {accuracy:.0%}\\n\")\n\nfor r in results:\n    mark = \"✓\" if r[\"correct\"] else \"✗\"\n    print(f\"{mark}  Q: {r['question']}\")\n    print(f\"   Expected: {r['expected']}  |  Extracted: {r['extracted']}\")\n    print(f\"   Model: {r['response'][:120]}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Try a prompt and compare\n",
    "\n",
    "Edit `MY_PROMPT` and re-run this cell to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MY_PROMPT = \"\"\"Solve the math expression and output only the number.\"\"\"\n\nsample = generate_problems(20)\nresponses = query_model_batch(MY_PROMPT, [p.question for p in sample])\nresults = []\n\nfor p, response in zip(sample, responses):\n    extracted = extract_number(response)\n    correct = check_answer(extracted, p.answer)\n    results.append({\n        \"question\": p.question,\n        \"expected\": p.answer,\n        \"response\": response.strip(),\n        \"extracted\": extracted,\n        \"correct\": correct,\n    })\n\naccuracy = sum(r[\"correct\"] for r in results) / len(results)\nprint(f\"Accuracy with prompt: {accuracy:.0%}\\n\")\n\nfor r in results:\n    mark = \"✓\" if r[\"correct\"] else \"✗\"\n    print(f\"{mark}  {r['question']}  →  expected {r['expected']}, got {r['extracted']}\")\n    if not r[\"correct\"]:\n        print(f\"   Model said: {r['response'][:120]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate across all test categories\n",
    "\n",
    "Run your prompt against the fixed test set and see per-category accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reuses MY_PROMPT from cell 4 — run that first\nfrom collections import defaultdict\n\ntest_problems = generate_test_problems(140)  # 20 per category\nby_cat = defaultdict(list)\nfor p in test_problems:\n    by_cat[p.category].append(p)\n\nprint(f\"{'Category':<18} {'Correct':>8} {'Total':>6} {'Acc':>6}\")\nprint(\"-\" * 42)\n\noverall_correct = 0\noverall_total = 0\n\nfor cat, problems in sorted(by_cat.items()):\n    responses = query_model_batch(MY_PROMPT, [p.question for p in problems])\n    correct = sum(\n        check_answer(extract_number(r), p.answer)\n        for p, r in zip(problems, responses)\n    )\n    acc = correct / len(problems)\n    overall_correct += correct\n    overall_total += len(problems)\n    print(f\"{cat:<18} {correct:>8} {len(problems):>6} {acc:>6.0%}\")\n\nprint(\"-\" * 42)\nprint(f\"{'TOTAL':<18} {overall_correct:>8} {overall_total:>6} {overall_correct/overall_total:>6.0%}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maths-prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}