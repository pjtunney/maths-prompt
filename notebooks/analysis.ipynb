{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maths-prompt: Progress Dashboard\n",
    "\n",
    "Visualise prompt optimisation progress across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "LOGS_DIR = Path(\"../logs\")\n",
    "EVAL_LOG = LOGS_DIR / \"evaluations.jsonl\"\n",
    "TEST_LOG = LOGS_DIR / \"test_results.jsonl\"\n",
    "\n",
    "def load_jsonl(path):\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame()\n",
    "    records = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df_train = load_jsonl(EVAL_LOG)\n",
    "df_test = load_jsonl(TEST_LOG)\n",
    "print(f\"Training evaluations: {len(df_train)}\")\n",
    "print(f\"Test evaluations: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy over iterations\n",
    "if not df_train.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    for session_id, group in df_train.groupby(\"session\"):\n",
    "        ax.plot(group[\"iteration\"], group[\"accuracy\"], marker=\"o\", label=f\"Session {session_id}\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"Training Accuracy Over Iterations\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training data yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt log â€” each attempt with its score\n",
    "if not df_train.empty:\n",
    "    prompt_log = df_train[[\"iteration\", \"session\", \"accuracy\", \"prompt\"]].copy()\n",
    "    prompt_log[\"prompt_preview\"] = prompt_log[\"prompt\"].str[:100] + \"...\"\n",
    "    display(prompt_log[[\"iteration\", \"session\", \"accuracy\", \"prompt_preview\"]])\n",
    "else:\n",
    "    print(\"No training data yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vs Training accuracy comparison\n",
    "if not df_train.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.plot(df_train[\"iteration\"], df_train[\"accuracy\"], marker=\"o\", label=\"Training\", alpha=0.7)\n",
    "    if not df_test.empty:\n",
    "        test_x = range(1, len(df_test) + 1)\n",
    "        ax.scatter(test_x, df_test[\"accuracy\"], marker=\"s\", s=100, c=\"red\", label=\"Test\", zorder=5)\n",
    "    ax.set_xlabel(\"Iteration / Eval\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"Training vs Test Accuracy\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category breakdown heatmap (from test results)\n",
    "if not df_test.empty and \"problems\" in df_test.columns:\n",
    "    rows = []\n",
    "    for idx, row in df_test.iterrows():\n",
    "        for p in row[\"problems\"]:\n",
    "            rows.append({\"eval\": idx + 1, \"category\": p[\"category\"], \"correct\": p[\"correct\"]})\n",
    "    df_cat = pd.DataFrame(rows)\n",
    "    pivot = df_cat.groupby([\"eval\", \"category\"])[\"correct\"].mean().unstack(fill_value=0)\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    im = ax.imshow(pivot.values.T, aspect=\"auto\", cmap=\"RdYlGn\", vmin=0, vmax=1)\n",
    "    ax.set_xticks(range(len(pivot.index)))\n",
    "    ax.set_xticklabels(pivot.index)\n",
    "    ax.set_yticks(range(len(pivot.columns)))\n",
    "    ax.set_yticklabels(pivot.columns)\n",
    "    ax.set_xlabel(\"Test Evaluation\")\n",
    "    ax.set_title(\"Accuracy by Category\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No test data with category breakdown yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best prompt found\n",
    "if not df_train.empty:\n",
    "    best_idx = df_train[\"accuracy\"].idxmax()\n",
    "    best = df_train.loc[best_idx]\n",
    "    print(f\"Best accuracy: {best['accuracy']:.1%} (iteration {best['iteration']}, session {best['session']})\")\n",
    "    print(f\"\\nPrompt:\\n{'='*60}\")\n",
    "    print(best[\"prompt\"])\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No training data yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
